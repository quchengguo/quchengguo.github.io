{"pages":[],"posts":[{"title":"Driver和Executor通信模拟","text":"Driver和Executor通信模拟，体会移动计算理念1.Driver端代码，driver类似客户端 1234567891011121314151617181920212223242526package com.study.bigdata.spark.core.commuimport java.io.{ObjectOutputStream, OutputStream}import java.net.{ServerSocket, Socket}/** * 演示driver提交任务到Executor */object Driver { def main(args: Array[String]): Unit = { // 建立连接 val client = new Socket(&quot;localhost&quot;, 9999) // 发送数据 val out: OutputStream = client.getOutputStream val objOut: ObjectOutputStream = new ObjectOutputStream(out) val task = new Task() objOut.writeObject(task) print(&quot;发送task&quot;) objOut.flush() objOut.close() // 关闭连接 client.close() }} 2.Executor端代码，executor类似服务端 1234567891011121314151617181920212223242526272829package com.study.bigdata.spark.core.commuimport java.io.{InputStream, ObjectInputStream}import java.net.{ServerSocket, Socket}/** * 演示driver提交任务到Executor */object Executor { def main(args: Array[String]): Unit = { // 启动服务 val server = new ServerSocket(9999) println(&quot;服务器启动，等待接受数据……&quot;) // 等待客户端连接 val client: Socket = server.accept() // 接受数据 val in: InputStream = client.getInputStream val inObj: ObjectInputStream = new ObjectInputStream(in) // 读取数据 val task: Task = inObj.readObject().asInstanceOf[Task] val res:List[Int] = task.compute() println(&quot;接受客户端数据：&quot; + res) // 关闭连接 inObj.close() in.close() client.close() server.close() }} 3.driver向executor提交的逻辑任务task 123456789101112131415package com.study.bigdata.spark.core.commu/** * 需要发送的类（逻辑，告诉executor怎么处理对应的数据） * 因为涉及到网络传输所以得继承序列化类 */class Task extends Serializable { val data = List(1, 2, 3, 4) val logic: (Int) =&gt; Int = _ * 2 def compute(): List[Int] = { data.map(logic) }} 4.先启动executor，后启动driver，结果如下","link":"/2022/03/19/Driver%E5%92%8CExecutor%E9%80%9A%E4%BF%A1%E6%A8%A1%E6%8B%9F/"},{"title":"Hadoop-RPC","text":"1.简介远程过程调用RPC（Remote Procedure Call）是一个计算机通信协议，最大优势是隐藏（简化）复杂的网络编程，调用远程服务就像调用本地服务一样，一般的分布式系统都实现自己的一套PRC。 2.基于Hadoop RPC Writable 实现一个简易的RPCHadoop RPC框架中序列化机制实现有两种： Avro Writable 接口实现，简单易懂 Google Protobuf 跨语言实现，跨语言，高扩展，高效率 下面通过Avro Writable实现RPC主要有4步骤： 定义接口（协议） 实现1中的接口 编写服务端 编写客户端（在客户端中请求服务端） 1.定义接口（协议） 123456789101112131415package hadoop.common.rpc;/** * 通信协议 */public interface BussinessProtocol { void mkdir(String path); String getName(String name); /** * 通信协议版本号 */ long versionID = 345043000L;} 2.实现1中的接口 1234567891011121314151617package hadoop.common.rpc;/** * 通信协议具体实现 */public class BussinessProtocolImpl implements BussinessProtocol { @Override public void mkdir(String path) { System.out.println(&quot;成功创建文件夹&quot; + path); } @Override public String getName(String name) { System.out.println(&quot;成功打了招呼: hello &quot; + name); return &quot;bigdata server&quot;; }} 3.编写服务端 12345678910111213141516171819202122232425262728package hadoop.common.rpc;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.ipc.RPC;import org.apache.hadoop.ipc.Server;import java.io.IOException;/** * 服务端 */public class MyServer { public static void main(String[] args) { try { /** * 提供一个服务 */ Server server = new RPC.Builder(new Configuration()) .setProtocol(BussinessProtocol.class) .setInstance(new BussinessProtocolImpl()) .setBindAddress(&quot;localhost&quot;) .setPort(6789) .build(); server.start(); }catch (IOException e){ e.printStackTrace(); } }} 4.编写客户端并请求服务端 12345678910111213141516171819202122232425262728package hadoop.common.rpc;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.ipc.RPC;import java.io.IOException;import java.net.InetSocketAddress;/** * 客户端 */public class MyClient { public static void main(String[] args) { try { BussinessProtocol proxy = RPC.getProxy(BussinessProtocol.class, BussinessProtocol.versionID , new InetSocketAddress(&quot;localhost&quot;, 6789), new Configuration()); /** * 交互，客户端获取到代理对象之后，直接调用服务端的方法，完全感觉不到发送了网络请求 */ String rpcResult = proxy.getName(&quot;my client&quot;); System.out.println(&quot;从RPC服务端收到getName响应结果为:&quot; + rpcResult); } catch (IOException e) { e.printStackTrace(); } }} 5.测试，先启动Server，后启动Client，观测打印日志","link":"/2022/03/16/Hadoop-RPC/"},{"title":"hello","text":"搭建博客踩过的坑 github认证由密码更改为token方式（token记得保管好） 安装icarus主题之后，hexo s启动本地服务之后，打开localhost:4000是一堆代码 需要安装对应的依赖 npm install --save bulma-stylus@0.8.0 hexo-renderer-inferno@^0.1.3 清除缓存，重新启动服务 切换next主题时遇到本地启动，打开localhost:4000是一堆代码。原因是hexo在5.0之后把swig给删除了需要自己手动安装npm i hexo-renderer-swig","link":"/2021/11/14/hello/"},{"title":"Spark的JOIN策略","text":"一、JOIN背景介绍Join是数据库查询永远绕不开的话题，传统查询SQL技术总体可以分为简单操作、聚合操作groupby以及join操作等。其中join操作是最复杂、代价最大的操作类型。也是olap场景中使用相对较多的操作。 二、基本实现机制spark中有5中join机制 Broadcast Hash Join Shuffle Hash Join Sort Merge Join Cartesian Join Broadcast Nested Loop Join 最常见的是前3个，其中Broadcast Hash Join、Shuffle Hash Join两者归根到底都属于hash join，只不过在hash前是做broadcast还是shuffle。其实，hash join算法来自传统数据库，而shuffle和broadcast是大数据的皮，两者结合之后就成了大数据的算法了。所以我们先来了解一下hash join的原理 2.1 hash join先来看这样一条sql select * from order,item where order.id=iterm.id 假设这个join采用hash join算法，过程图如下： 整个过程会经历三步： 确定build table基础表以及probe table探测表：基础表使用join的key构建一个hash table，探测表使用join的key进行探测，探测成功就join在了一起。通常情况下，小表会做为基础表（因为会放在内存中），大表会做为探测表，该示例中iterm为基础表，order表为探测表 构建Hash Table：依次读取基础表的数据，对于每一行数据根据join key进行hash，hash到对应的bucket，生成hash table中的一条记录，数据缓存在内存中，如果内存放不下就dump到外存 探测：扫描探测表的数据，使用相同的hash函数映射Hash Table中的记录，映射成功之后再检查join的条件（order.id=iterm.id），如果匹配成功就可以将两者join在一起 2.2 broadcast hash join将其中一张小表广播分发到一张大表所在的节点上，分别并发的与其上的分区记录进行hash join。适合表很小直接可以广播的场景 broadcast阶段：将小表广播到大表所在的节点。最简单的是先发给driver然后driver统一分发给所有的executor hash join阶段：在每个executor上执行2.1中的hash join，小表做为基础表，大表做为探测表 broadcast hash join执行的基本条件为被广播小表必须小于参数 spark.sql.autoBroadcastJoinThreshold，默认为10M 2.3 shuffle hash join在大数据场景下如果一张表很小那么选择broadcast hash join无疑是效率最高的，但是一旦小表数据量增大，广播所需内存、带宽等资源就会太大，broadcast hash join就不再是最优方案。此时，shuffle hash join就是比较合适的方案，先按照key进行分区（此过程会产生shuffle ），就可以将大表join分而治之，划分为很多小表的join，充分利用分布式的优点。 shuffle 阶段：将两个表join的key进行分区，那么相同的key将会分配到同一个节点 hash join阶段：每个节点上进行hash join算法 可以看出该算法也用到了hash join意味着节点上的小表也是放在内存中，若节点上的表都是大表那么该算法就不适用了 2.4 sort merge join如果两个大表进行join，spark采用了sort merge join算法 shuffle阶段：将两张大表根据join的key进行shuffle sort阶段：对单个分区节点的两张表数据，分别进行排序 merge阶段：对排好序的两张表执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同的key就merge输出 三、总结经过上文的分析，很明显可以看出来几种join的代价：broadcast hash join&lt;shuffle hash join&lt;sort merge join，所以优化sql执行时优先以这个顺序去考虑","link":"/2022/07/30/Spark%E7%9A%84JOIN%E7%AD%96%E7%95%A5/"},{"title":"flink重要概念","text":"一、Flink组件1.JobManager作为主进程，JobManager控制着单个应用程序的执行。JobManager可以接收所需要执行的应用，该应用会包含一个JobGraph，即逻辑DataFlow图，以及一个打包的Jar文件。JobManager将JobGraph转换为ExecutionGraph的物理DataFlow图，该图包含了哪些可以并行执行的任务。JobManager从ResourceManager申请必要的执行资源（槽-TaskManager的slot），然后将ExecutionGraph中的任务分发给TaskManger来执行。在此过程中，JobManager还负责所有需要集中协调的操作，如创建checkpoint。 总的来说有两点： 提交程序 创建checkpoint 2.ResourceManagerResourceManger负责管理flink的处理资源单元—-槽。 总的来说两点： 分配资源 释放计算资源 3.TaskManagerTaskManager是flink的工作进程。通过会启动多个TaskManager，每个TaskManger有多个槽。槽的个数限制了TaskManager可并行执行的任务数。TaskManager在启动后，会向ResourceManger注册它的槽，当接收到ResourceManger的指示时，TaskManager会向JobManager提供一个或多个槽。在执行期间，运行同一应用不同任务的TaskManager之间会产生数据交换。 总的来说三点： 提供槽 执行任务 同应用交换数据 4.Dispatcher提供rest接口让我们提交应用程序。一旦提交执行，dispatcher会启动一个JobManager并转交给它。 5.图示组件关系 二、任务执行下图展示TaskManager、处理槽（slot）、任务以及算子之间的关系 左侧的JobGraph包含了5个算子（右下角标为并行度），其中算子A和C是数据源。由于算子最大并行度为4，因此至少需要4个槽（slot）来处理。若每个TaskManager内有两个槽（slot），则需要运行两个TaskManager可满足该任务需求。JobManager将JobGraph【展开成】ExecutionGraph并把任务分配到4个空闲的处理槽。将任务以切片的形式调度至处理槽中的好处是TaskManager中的多个任务可以在同一个进程内高效地执行数据交换而无须访问网络。 三、Flink降低任务通信开销技术通过网络连接逐条发送记录低效且会导致很多额外的开销，若想充分利用网络带宽，就需要进行数据缓冲。如何缓冲flink通过如下两种方式实现。 1.基于信用值（credit）的流量控制发送任务（Sender）和接受任务（Receiver）通过互相告知对方自己的处理能力的方式来精准地进行流控 2.任务链接四、时间时间处理五、状态管理","link":"/2022/08/30/flink%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5/"},{"title":"hive udaf","text":"一、问题点12345678910111213141516171819202122232425select class ,collect_set(concat_ws(',',movie_arr)) as movie_arrfrom(select name,class,collect_set(concat_ws(',',movie)) as movie_arrfrom(select '张三' as name,'天下无贼' as movie,1 as classunion all select '张三' as name,'霸王别姬' as movie,1 as classunion all select '李四' as name,'霸王别姬' as movie,1 as classunion all select '王五' as name,'放牛班的春天' as movie,2 as class)group by name,class)class; 若遇到以上问题，需求是必须先聚合到明细name+class，然后再聚合到class，这时候在最外层使用collect_set就会报错。 此时，需要实现一个uadf能在数组间进行元素去重。 二、解决办法以下是uadf开发流程 2.1 新建Java项目，配置maven文件12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.10.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;2.3.7&lt;/version&gt;&lt;/dependency&gt; 2.2 开发功能继承AbstractGenericUDAFResolver类 2.2.1 关于UDAF的四个阶段在编码前，要先了解UDAF的四个阶段，定义在GenericUDAFEvaluator的Mode枚举中： 1.COMPLETE:如果MapReduce只有map而没有reduce，就会进入这个阶段 2.PARTIAL1:正常MapReduce的map阶段 3.PARTIAL2:正常MapReduce的combiner阶段 4.FINAL:正常MapReduce的reduce阶段 2.2.2 每个阶段被调用的方法 开发udaf时，要继承抽象类GenericUDAFEvaluator，里面有多个抽象方法，在不同的阶段，会调用到这些方法中的一个或多个 下图对每个阶段调用了哪些方法说的很清楚： 下图对顺序执行的三个阶段和涉及方法做了详细说明 下面是对应的代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120import java.io.Serializable;import java.util.ArrayList;import java.util.Collection;import java.util.LinkedHashSet;import java.util.List;import org.apache.hadoop.hive.ql.exec.Description;import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.hive.ql.parse.SemanticException;import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;import org.apache.hadoop.io.Text;@Description(name = &quot;collect_split_set&quot;, value = &quot;_FUNC_(x) - Returns a set of objects with duplicate elements eliminated&quot;)public class CollectSplitSet extends AbstractGenericUDAFResolver { public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException { if (parameters.length != 1) throw new UDFArgumentTypeException(parameters.length - 1, &quot;Exactly one argument is expected.&quot;); if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) throw new UDFArgumentTypeException(0, &quot;Only primitive type arguments are accepted but &quot; + parameters[0] .getTypeName() + &quot; was passed as parameter 1.&quot;); return new MkCollectionSizeEvaluator(); } public static class MkCollectionSizeEvaluator extends GenericUDAFEvaluator implements Serializable { private static final long serialVersionUID = 1L; private transient PrimitiveObjectInspector inputOI; private transient StandardListObjectInspector loi; private transient ListObjectInspector internalMergeOI; public ObjectInspector init(GenericUDAFEvaluator.Mode m, ObjectInspector[] parameters) throws HiveException { super.init(m, parameters); if (m == GenericUDAFEvaluator.Mode.PARTIAL1) { this.inputOI = (PrimitiveObjectInspector)parameters[0]; return (ObjectInspector)ObjectInspectorFactory.getStandardListObjectInspector( ObjectInspectorUtils.getStandardObjectInspector((ObjectInspector)this.inputOI)); } if (!(parameters[0] instanceof ListObjectInspector)) { this .inputOI = (PrimitiveObjectInspector)ObjectInspectorUtils.getStandardObjectInspector(parameters[0]); return (ObjectInspector)ObjectInspectorFactory.getStandardListObjectInspector((ObjectInspector)this.inputOI); } this.internalMergeOI = (ListObjectInspector)parameters[0]; this.inputOI = (PrimitiveObjectInspector)this.internalMergeOI.getListElementObjectInspector(); this.loi = (StandardListObjectInspector)ObjectInspectorUtils.getStandardObjectInspector((ObjectInspector)this.internalMergeOI); return (ObjectInspector)this.loi; } class MkSetAggregationBuffer extends GenericUDAFEvaluator.AbstractAggregationBuffer { private Collection&lt;Object&gt; container = new LinkedHashSet(); } public GenericUDAFEvaluator.AggregationBuffer getNewAggregationBuffer() throws HiveException { return (GenericUDAFEvaluator.AggregationBuffer)new MkSetAggregationBuffer(); } public void reset(GenericUDAFEvaluator.AggregationBuffer agg) throws HiveException { ((MkSetAggregationBuffer)agg).container.clear(); } public void iterate(GenericUDAFEvaluator.AggregationBuffer agg, Object[] parameters) throws HiveException { assert parameters.length == 1; Object p = parameters[0]; if (p != null) { MkSetAggregationBuffer myagg = (MkSetAggregationBuffer)agg; String[] strings = String.valueOf(p).split(&quot;,&quot;); for (String str : strings) { if (!&quot;&quot;.equals(str)) putIntoCollection(new Text(str), myagg); } } } private void putIntoCollection(Object p, MkSetAggregationBuffer myagg) { Object pCopy = ObjectInspectorUtils.copyToStandardObject(p, (ObjectInspector)this.inputOI); myagg.container.add(pCopy); } public Object terminatePartial(GenericUDAFEvaluator.AggregationBuffer agg) throws HiveException { MkSetAggregationBuffer myagg = (MkSetAggregationBuffer)agg; List&lt;Object&gt; ret = new ArrayList(myagg.container.size()); ret.addAll(myagg.container); return ret; } public void merge(GenericUDAFEvaluator.AggregationBuffer agg, Object partial) throws HiveException { MkSetAggregationBuffer myagg = (MkSetAggregationBuffer)agg; List&lt;Object&gt; partialResult = this.internalMergeOI.getList(partial); if (partialResult != null) for (Object i : partialResult) putIntoCollection(i, myagg); } public Object terminate(GenericUDAFEvaluator.AggregationBuffer agg) throws HiveException { MkSetAggregationBuffer myagg = (MkSetAggregationBuffer)agg; List&lt;Object&gt; ret = new ArrayList(myagg.container.size()); ret.addAll(myagg.container); return ret; } }}和collect_set的区别：String[] strings = String.valueOf(p).split(&quot;,&quot;); // 多一步解开元素的操作for (String str : strings) { if (!&quot;&quot;.equals(str)) putIntoCollection(new Text(str), myagg); } 2.3 上传hdfs2.4 添加jar add jar hdfs://xxx/myfun-1.0-SNAPSHOT.jar； 2.5 创建函数create function func_name as “类路径”; 2.6使用select func_name()","link":"/2022/07/24/hive-udf/"},{"title":"mac搭建单机flink并运行helloworld","text":"准备好好开始学习flink，先从hello world开始吧 1.安装flink本文介绍通过brew安装flink 1.1. 安装首先查看是否安装flink使用 brew list | grep flink查看，若安装进行启动步骤，若未安装使用brew install openjdk@11; brew install apache-flink指令安装（该过程可能比较漫长，我因为网络不好导致多次下载中断） 1.2启动进入/opt/homebrew/Cellar/apache-flink/1.14.0执行./libexec/bin/start-cluster.sh，然后在浏览器访问http://localhost:8081 会显示如下界面 2.运行helloworld在idea中创建maven项目，加入如下代码 12345678910111213141516171819202122232425262728293031323334353637383940414243public class SocketTextStreamWordCount { public static void main(String[] args) throws Exception { //参数检查 if (args.length != 2) { System.err.println(&quot;USAGE:\\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;&quot;); return; } String hostName = args[0]; Integer port = Integer.parseInt(args[1]); //设置环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据 DataStream&lt;String&gt; text = env.socketTextStream(hostName, port); //计数 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new LineSplitter()) .keyBy(0) .sum(1); counts.print(); env.execute(&quot;Java WordCount from SocketTextStream Example&quot;); } public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) { String[] tokens = s.toLowerCase().split(&quot;\\\\W+&quot;); for (String token: tokens) { if (token.length() &gt; 0) { collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } } }} pom.xml 12345678910111213141516171819202122232425&lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;flink.version&gt;1.13.0&lt;/flink.version&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;slf4j.version&gt;1.7.30&lt;/slf4j.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; 然后打包程序 开启监听端口 nc -l 9000 最后在后台运行 ./bin/flink run -c com.atguigu.wc.SocketTextStreamWordCount /Users/guozaizai/IdeaProjects/guigu_flink/target/guigu_flink-1.0-SNAPSHOT.jar 127.0.0.1 9000，此时我们可以在界面看到正在运行的程序（如果报ip解析错误在flink-conf.yaml中配置taskmanager.host: localhost） 之后我们可以在nc中输入text，然后可以通过tail -f ./libexec/log/flink-xxx-taskexecutor-1-xxx-Pro.local.out看到统计结果 参考：https://www.jianshu.com/p/17676d34dd35","link":"/2022/08/21/mac%E6%90%AD%E5%BB%BA%E5%8D%95%E6%9C%BAflink%E5%B9%B6%E8%BF%90%E8%A1%8Chelloworld/"},{"title":"数据仓库模型-星型,雪花,星座","text":"一、什么是数据仓库模型？模型是描述整个数据库的逻辑描述，包括记录的名称和描述。它具有所有数据项以及数据关联的不同聚合 二、数据仓库模型的类型事实表和维度表构成了数据仓库中任何模型的基础，这些表和维度表对于理解很重要。 事实表，应该具有域任何业务流程对应的数据。每行代表可以与任何进程关联的任何事件。它存储用于分析的定量信息。 维度表，存储有关如何分析事实表的数据的数据（实体）。它们有助于事实表收集有关将要采取的措施的不同维度。 2.1 星型模型所有维表都直接连接到“事实表”上时，整个图就像星星一样，故将该模型称为星型模型，如图 星型模型是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连，不存在渐变维度，所以数据有一定的冗余。 如：在地域表中，存在国家A省B城市C以及国家A省B城市D两条记录，那么国家A和省B的信息分别存储了两次，即存在冗余 2.1.1 星型模型的特点 在星型模式中，一个事实表和若干个维度表连接，这种结构类似于星星，因此被称为星形模式。 这里的事实表由数据仓库中的主要信息组成。它围绕较小的维度查找表，这些表将包含不同事实表的详细信息。每个维度中存在的主键与事实表中的外键相关 事实表采用3NF形式，维度表采用非规范化形式。星型模式中的每个维度都应该由唯一的维度表表示。维度表应该连接到一个事实表。事实表应该有一个键和属性。 2.2 雪花模型当有一个或多个维度表没有直接连接到事实表上，而是通过其他维度表连接到事实表上时，其图解就像很多个雪花连接在一起，故称为雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各个维表可能被扩展为更小的维度表，形成一些局部的“层次区域”，这些被分解的表都连接到主维度表而不是事实表。如图 将地域表分解为国家，省份，城市等维表。它的优点是：通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花结构去除了数据冗余 星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率要比雪花模型要高。星型结构不用考虑很多正规化的因素，设计与实现比较简单。雪花模型去除了冗余，有些统计通过表关联才能产生，所以效率不一定有雪花模型高。正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的ETL、以及后期的维护都要复杂一些。因此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率 2.3 星座模型（事实星座模型）星座模型是由星型模型延伸而来，星型模型是基于一张事实表而星座模型是基于多张事实表，并且共享维度表的信息，这种模型往往应用于数据关系比星型模型和雪花模型更复杂的场合。星座模型需要多个事实表共享维度表，因而可以视为星型模型的集合，因此称为星座（系）模型 从本质上讲，可以讲星座模型拆分为互相关联且完全标准化的星型模型的集合，以避免数据的冗余和不准确。也可以是雪花模型与星型模型的关联，两个模型的事实表关联，根据需要连接两种结构的维度表。由此产生的系统在结构中看起来像是一个星座，其中事实表是这里的星星。这解释了名称Galaxy模型。这种类型的模型也称为“事实星座”模型，因为他有多个事实表。 2.3.1如何创建星座模型当系统由连接到维度表C、D、E、F、G、H、I的事实表A和B组成时，可以构建Galaxy Schema。根据规则，事实表可以相互连接，维度表可以与系统内部的任何事实表和维度表连接，下面的示例将有助于更好地理解Galaxy Schema的底层概念。 在这个例子中，学生数据库有两个事实表-学生和教职员工。两个事实表都可以有共同维度—专业课程、艺术与科学，基于各自部门。此外，从学生角度来看，标准化适用于获得合作社和校外工作。这可以进一步细分以提及经验类型的实习。 另一方面，教学人员会给学生布置作业。并且，这些分配维度表可以进一步规范化以定义评估细节。所有这些维度表都可以根据项目提供给设计团队的信息的局限性进一步规范化。 2.3.2Galaxy Schema的优缺点优点： 它的多维特性有助于有效地构建复杂的数据库系统 作为标准化的结果，最小或没有冗余 考虑到系统的复杂性，这个一个灵活的模式 数据质量会很好，因为规范化为定义明确的表/数据格式提供了优势 高质量和准确性有助于创建出色的报告和分析结果 缺点： Galaxy模型在结构上可能很复杂 处理这个模型是乏味的，因为模型和数据库系统的复杂性使它变得更加复杂 数据检索是通过结合条件表达式的多级连接完成的 预期的标准化级别取决于给定数据库的深度 由于galaxy模型用于具有复杂结构的大型数据库系统，因此维护和支持任务变得困难 其较大的设计布置和详细的查询过程需要较大的存储空间 分析变得困难，因为它对可以拥有的事实表和维度表的数量没有限制 2.4 对比2.4.1雪花 vs 星型维度的层级，标准的星型模型的维度层级只有一层，而雪花星型的维度可能涉及到多层 2.4.2星型 vs 星座、雪花星座模型基本上是很多数据仓库的常态，因为很多数据仓库都是多个事实表的。所以星座不是星座只反映是否多事实表，他们之间是否共享一些维度表。 2.4.3星型 vs 雪花 vs 星座 特征 星型架构 雪花架构 星座模型 维护/更改 它有更多的冗余数据，因此更难更改或维护 由于冗余较少，此模式更易于更改和维护 冗余少，但是结构复杂，难以实现 易懂 查询的复杂度较低，因此很容易理解 应用的查询更复杂，因此难以理解 复杂 查询执行 时间 它有更少外键，因此查询执行速度更快，花费时间更少 由于外键较多，查询执行时间较多，或者查询执行速度较慢 数据仓库类型 更适合具有单一关系的数据集市，即一对一或一对多 更适合复杂的关系，即多对多 复杂的应用程序 连接数 它有更多的连接数 它有更少的连接数 多 维度表 每个维度表只有一个维度表 有一个或多个维度表 可用性 如果维度表的大小较小，即行数较少，则首选星型模式 当维度表较大时很好用 规范化和非规范化 事实表和维度表都是非规范化的 事实表规范，而维度表都是非规范化的 复杂 数据模型 它遵循自上而下的方法 自上而下 三、模型的选择首先就是星座不是星座这个只跟数据和需求有关系，跟设计没关系，不用选择 星型还是雪花，取决于性能优先还是避免冗余、灵活度优先 目前实际企业开发中，不会绝对选择一种，根据情况灵活组合，甚至并存。但整体来看，更倾向于维度更少的星型模型。尤其是hadoop体系，减少join就是减少shuffle，性能差距很大 3.1 数据优化雪花模型能消除冗余，有效地减少数据量 3.2 业务模型主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID 就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID 将是 Account_dimension 的一个外键。 在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。 3.3 性能雪花模型join多性能低。 星型模型join少性能高。 3.4 ETL四、总结雪花模型使得维度分析更加容易，比如”针对特定的广告主，有哪些客户或者公司是在线的“ 星型模型用来做指标分析更合适，比如”一个客户他们的收入是多少“ 事实星座模式是数据仓库最长使用的数据模式，尤其是企业级数据仓库（EDW）。这也是数据仓库区别于数据集市的一个典型的特征，从根本上而言，数据仓库数据模型的模式更多是为了避免冗余和数据复用，套用现成的模式，是设计数据仓库最合理的选择。当然大数据技术体系下，数据仓库数据模型的设计，还是一个盲点，探索中","link":"/2022/07/20/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A8%A1%E5%9E%8B-%E6%98%9F%E5%9E%8B-%E9%9B%AA%E8%8A%B1-%E6%98%9F%E5%BA%A7/"},{"title":"罗素：我的人生追求","text":"伯特兰·罗素：我的人生追求&nbsp;&nbsp;有三种简单而无比强烈的激情左右了我的一生：对爱的渴望，对知识的探索和对人类苦难的难以忍受的怜悯。这些激情像飓风，无处不在、反复无常地吹拂着我，吹过深重的苦海，濒与绝境。 &nbsp;&nbsp;我寻找爱，首先是因为它使人心醉神迷，这种陶醉是如此的美妙，使我愿意牺牲所有的余生去换取几个小时这样的欣喜。我寻找爱，还因为它能解除孤独，在可怕的孤独中，一颗颤抖的灵魂从世界的边缘看到冰冷、无底、死寂的深渊。最后，我寻找爱，还因为在爱的交融中，神秘而又具体入微地，我看到了圣贤和诗人们想象出的天堂的前景。这就是我所寻找的，而且，虽然对人生来说似乎过于美妙，这也是我终于找到了的。 &nbsp;以同样的激情我探索知识，我希望能够了解人类的心灵。我希望能够知道群星为何闪烁。我试图领悟毕达哥拉斯所景仰的数字的力量，它支配着此消彼长。仅在不大的程度上，我达到了此目的。 &nbsp;爱和知识，只要有可能，通向着天堂。但是怜悯总把我带回尘世。痛苦呼喊的回声回荡在我的内心。忍饥挨饿的孩子，惨遭压迫者摧残的受害者，被儿女们视为可憎的无助的老人，连同这整个充满了孤独、贫穷和痛苦的世界，使人类所应有的生活成了笑柄。我渴望能够减少邪恶，但是我无能为力，而且我自己也在忍受折磨。 &nbsp;这就是我的一生，我发现它值得一过。如果在给我一次机会，我会很高兴地再活它一次。","link":"/2021/12/20/%E7%BD%97%E7%B4%A0%EF%BC%9A%E6%88%91%E7%9A%84%E4%BA%BA%E7%94%9F%E8%BF%BD%E6%B1%82/"},{"title":"一切","text":"一切一切都是命运 一切都是烟云 一切都是没有结局的开始 一切都是稍纵即逝的追寻 一切欢乐都没有微笑 一切苦难都没有泪痕 一切语言都是重复 一切交往都是初逢 一切爱情都在心里 一切往事都在梦中 一切希望都带着注释 一切信仰都带着呻吟 一切爆发都有片刻的宁静 一切死亡都有冗长的回声","link":"/2022/02/16/%E4%B8%80%E5%88%87/"},{"title":"单机版flink接收kafka消息","text":"1.版本对应在调试过程中，因为版本不一致一直报错Could not create actor system，最后版本对应起来就能正常消费了。 flink版本：1.13.0 scala版本：2.12 kafka版本：kafka_2.12-3.2.1（2.12是scala版本，3.2.1是kafka版本） 2.下载kafka下载地址：https://downloads.apache.org/kafka/3.2.1/kafka_2.12-3.2.1.tgz 其中kafka自带zk不需要单独下载zk 3.发送消息 启动zk：bin/zookeeper-server-start.sh config/zookeeper.properties 启动kafka：bin/kafka-server-start.sh config/server.properties 创建topic：bin/kafka-topics.sh --create --topic test_flink_topic1059 --bootstrap-server localhost:9092 生产数据：bin/kafka-console-producer.sh --topic test_flink_topic1059 --bootstrap-server localhost:9092 4.flink接受消息Pom.xml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;flink.version&gt;1.13.0&lt;/flink.version&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt; &lt;slf4j.version&gt;1.7.30&lt;/slf4j.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--流处理--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 引入日志管理相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;${slf4j.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;version&gt;2.14.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.10.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;2.3.7&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接2.12scala版本的kafka:kafka_2.12-3.2.1 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; java程序 1234567891011121314151617181920212223242526272829303132package com.atguigu.wc;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;import java.util.Properties;public class KafkaToFlink { public static void main(String[] args) throws Exception { // 创建流处理的执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 配置KafKa //配置KafKa和Zookeeper的ip和端口 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;); properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); //将kafka和zookeeper配置信息加载到Flink的执行环境当中StreamExecutionEnvironment FlinkKafkaConsumer myConsumer = new FlinkKafkaConsumer(&quot;test_flink_topic1059&quot;, new SimpleStringSchema(), properties); //添加数据源，此处选用数据流的方式，将KafKa中的数据转换成Flink的DataStream类型 DataStream stream = env.addSource(myConsumer); //打印输出 stream.print(); //执行Job，Flink执行环境必须要有job的执行步骤，而以上的整个过程就是一个Job env.execute(&quot;kafka sink test&quot;); }} 之后在kafka生产数据，就可以看到idea控制台有消息打印","link":"/2022/08/29/%E5%8D%95%E6%9C%BA%E7%89%88flink%E6%8E%A5%E6%94%B6kafka%E6%B6%88%E6%81%AF/"},{"title":"icarus主题配置","text":"取消侧边栏不需要的挂件：在_config.icarus.yml的widgets元素中注释掉形如下面的代码 1234-position:type:xxx: 示例：注意-这个特殊字符也要注释掉否则hexo g的时候会报错 2.上传图片使用图床https://sm.ms/","link":"/2021/12/28/icarus%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"join","slug":"join","link":"/tags/join/"},{"name":"flink","slug":"flink","link":"/tags/flink/"},{"name":"udf","slug":"udf","link":"/tags/udf/"},{"name":"数仓","slug":"数仓","link":"/tags/%E6%95%B0%E4%BB%93/"},{"name":"人文","slug":"人文","link":"/tags/%E4%BA%BA%E6%96%87/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"}],"categories":[]}